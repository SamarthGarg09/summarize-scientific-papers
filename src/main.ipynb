{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip -q install accelerate\n","!apt-get install git-lfs\n","!git lfs install\n","!pip -q install evaluate\n","!pip -q install rouge_score"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import nltk\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')\n","os.environ['TOKENIZERS_PARALLELISM']='false'\n","\n","import torch\n","from torch.utils.data import DataLoader\n","\n","import evaluate\n","from datasets import Dataset, load_dataset, DatasetDict\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"/kaggle/input/extract-scien-summ/extracted_summary.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df['text_len'] = df.apply(lambda x: len(x['text'].split()), axis = 1)\n","df.text_len.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ds = load_dataset(\"csv\", data_files=\"/kaggle/input/extract-scien-summ/extracted_summary.csv\")\n","ds = ds['train'].train_test_split(test_size=0.01)\n","ds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set_style(\"darkgrid\")\n","sns.set_palette(\"muted\")\n","sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n","\n","def plot_hist(data, title, xlabel, ylabel):\n","    plt.figure(figsize=(16, 5))\n","    plt.hist(data, bins=100, color='g')\n","    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n","    plt.show()\n","\n","plot_hist([len(x.split()) for x in ds['train']['text']], 'Text Length Distribution', 'Length', 'Number of Samples')\n","plot_hist([len(x.split()) for x in ds['train']['summary']], 'Summary Length Distribution', 'Length', 'Number of Samples')\n","plot_hist([len(x.split()) for x in ds['train']['extracted_summary']], 'Extracted Summary Length Distribution', 'Length', 'Number of Samples')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_checkpoint = \"facebook/bart-large-cnn\"\n","tokenizer = BartTokenizer.from_pretrained(model_checkpoint)\n","\n","max_input_length = 1024\n","max_target_length = 150\n","\n","def tokenize(example):\n","    model_inputs = tokenizer(\n","        example['extracted_summary'],\n","        truncation=True,\n","        max_length = max_input_length\n","    )\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(\n","            example['summary'],\n","            truncation=True,\n","            max_length = max_target_length\n","        )\n","    model_inputs['labels'] = labels['input_ids']\n","    return model_inputs\n","\n","tokenized_ds = ds.map(tokenize, batched=True, remove_columns=ds['train'].column_names)\n","tokenized_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq, BartForConditionalGeneration \n","\n","model = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataloader = DataLoader(\n","    tokenized_ds[\"train\"], shuffle=True, collate_fn=data_collator, batch_size=8\n",")\n","eval_dataloader = DataLoader(\n","    tokenized_ds[\"test\"], collate_fn=data_collator, batch_size=8\n",")\n","\n","for batch in train_dataloader:\n","    break\n","print({k:v.shape for k, v in batch.items()})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import get_scheduler\n","\n","num_epochs = 10\n","num_training_steps = num_epochs * len(train_dataloader)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import wandb\n","\n","wandb.init(project=\"summarization\", config={\"learning_rate\": 5e-5, \"epochs\": num_epochs, \"batch_size\": 8})\n","wandb.watch(model, log=\"all\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login, Repository, get_full_repo_name\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from accelerate import Accelerator\n","from huggingface_hub import notebook_login, Repository, get_full_repo_name\n","\n","notebook_login()\n","repo_name = get_full_repo_name(\n","    \"bart-large-cnn-finetuned-scientific_summarize\",\n","    organization = \"SmartPy\"\n",")\n","output_dir = \"./output\"\n","repo = Repository(output_dir, clone_from=repo_name)\n","\n","accelerate = Accelerator()\n","model, optimizer, train_dataloader, eval_dataloader = accelerate.prepare(\n","    model, optimizer, train_dataloader, eval_dataloader\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import load_metric\n","rouge_score = load_metric(\"rouge\")\n","\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [[label.strip()] for label in labels]\n","    \n","    # metrics expects newline after each sentence\n","    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n","    labels = [[\"\\n\".join(nltk.sent_tokenize(label))] for label in labels]\n","    return preds, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","progress_bar = tqdm(range(num_training_steps))\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        accelerate.backward(loss)\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)\n","        wandb.log({\"train_loss\": loss.item()})\n","    model.eval()\n","    for step, batch in enumerate(eval_dataloader):\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            wandb.log({\"eval_loss\": loss.item()})\n","            generated_tokens = accelerate.unwrap_model(model).generate(\n","                batch[\"input_ids\"],\n","                attention_mask=batch[\"attention_mask\"],\n","            )\n","\n","            generated_tokens = accelerate.pad_across_processes(\n","                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n","            )\n","            labels = batch[\"labels\"]\n","\n","            # If we did not pad to max length, we need to pad the labels too\n","            labels = accelerate.pad_across_processes(\n","                batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n","            )\n","\n","            generated_tokens = accelerate.gather(generated_tokens).cpu().numpy()\n","            labels = accelerate.gather(labels).cpu().numpy()\n","\n","            # Replace -100 in the labels as we can't decode them\n","            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","            if isinstance(generated_tokens, tuple):\n","                generated_tokens = generated_tokens[0]\n","            decoded_preds = tokenizer.batch_decode(\n","                generated_tokens, skip_special_tokens=True\n","            )\n","            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","            decoded_preds, decoded_labels = postprocess_text(\n","                decoded_preds, decoded_labels\n","            )\n","\n","            metrics.add_batch(predictions=decoded_preds, references=decoded_labels)\n","\n","    # Compute metrics\n","    result = metrics.compute()\n","    # Extract the median ROUGE scores\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","    result = {k: round(v, 4) for k, v in result.items()}\n","    print(f\"Epoch {epoch}:\", result)\n","\n","    # Save and upload\n","    accelerate.wait_for_everyone()\n","    unwrapped_model = accelerate.unwrap_model(model)\n","    unwrapped_model.save_pretrained(output_dir, save_function=accelerate.save)\n","    if accelerate.is_main_process:\n","        tokenizer.save_pretrained(output_dir)\n","        repo.push_to_hub(\n","            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["accelerate.remove_slurm_checkpoint()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","from transformers import pipeline\n","\n","hub_model_id = \"SmartPy/bart-large-cnn-finetuned-scientific_summarize\"\n","summarizer = pipeline(\"summarization\", model=hub_model_id)\n","\n","def summarize_research_papers(idx):\n","    review = ds[\"test\"][idx][\"extracted_summary\"]\n","    title = ds[\"test\"][idx][\"summary\"]\n","    summary = summarizer(ds[\"test\"][idx][\"review_body\"])[0][\"summary_text\"]\n","    print(f\"'>>> Paper: {review}'\")\n","    print(f\"\\n'>>> Summary: {title}'\")\n","    print(f\"\\n'>>> Generated Summary: {summary}'\")\n","\n","summarize_research_paper(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import shutil \n","# shutil.rmtree(\"/kaggle/working/bart-large-cnn-finetuned-scientific_summarize/checkpoint-1500\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"},"vscode":{"interpreter":{"hash":"ccf8f30e77093729742405d54c9e75678c70bdc964609122f3fc0f9f893184f4"}}},"nbformat":4,"nbformat_minor":4}
